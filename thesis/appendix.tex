\subsection{Linear Algebra}

\subsubsection{Vector Spaces}

\begin{definition}
    Let $F$ be a field. Let $V$ be a subset of $F$, and suppose that $V$
    satisfies the following properties
    \begin{enumerate}[i]
        \item $v_1, v_2 \in V \implies v_1 + v_2 \in V$
        \item $v_1 \in V, c \in F \implies c * v_1 \in V$
    \end{enumerate}
    In this case, $V$ is said to be a \emph{vector space} over $F$.
\end{definition}

\subsubsection{Matrix Representation of a Linear Transformation}

\subsubsection{Determinants}

\leavevmode \\

\todo[inline]{Include recursive formula as given in \cite{artin2014algebra}}

% \subsubsection{Row Echelon Form}
% 
% The setting in which one wants to produce an upper triangular matrix usually involves some desire to solve the equation
% \begin{equation*}
%   AX = B
% \end{equation*}
% where $A$ is an invertible linear transformation, and $X, B$ are vectors (or elements of a free module). One can perform \emph{Gaussian Elimination} to produce an upper triangular matrix (or \emph{Gauss-Jordan Elimination} to produce the reduced row echelon form of a matrix) by applying a sequence of elementary row operations to the system. \\
% 
% Now, these elementary row operations can be represented as elementary matrices, say $E_1, \ldots, E_k$. Applied to the original system, we have
% \begin{align*}
%   E_k \cdots E_1 AX &= E_k \cdots E_1 B\\
%   A'X &= B'
% \end{align*}
% 
% We know that these operations do not change the solution to the equation by application of the cancellation law, since elementary matrices are invertible. Then, one may obtain $X$ by back-substitution. If $A$ was not invertible, then a pseudoinverse may be obtained to describe all solutions that satisfy the equation.

\subsection{Rings}
\label{seq:rings}

\begin{definition}
  A \emph{ring} is a set $R$ with two binary operations $+, \times$, called addition and multiplication respectively, that satisfy the following properties:
  \begin{itemize}
  \item $R, +$ is an abelian group
  \item $R, *$ is a group
  \item Distributive property
    \begin{itemize}
    \item Left Distributive: $a(b + c) = ab + ac$
    \item Right Distributive: $(a + b)c = ac + bc$
    \end{itemize}
  \end{itemize}
\end{definition}

A ring in which multiplication is commutative is called a \emph{commutative ring}.

\leavevmode \\

\todo[inline]{Discuss subrings}

\begin{definition}
  An \emph{ideal} $I$ is a subset of a ring $R$ that satisfies the following properties:
  \begin{itemize}
  \item $0 \in I$
  \item If $a, b \in I$, then $a + b \in I$
  \item If $a \in I, c \in R$, then $ca \in I$
  \end{itemize}
\end{definition}

Given an element $a$ of a ring $R$, the set $(a)$ containing all multiples of $a$ is an ideal, i.e. $0*a = 0 \in (a)$, $a + a = 1*a + 1*a = (1 + 1)*a \in (a)$. In this case, $(a)$ is said to be the \emph{principal ideal generated by $a$}.

\todo[inline]{Discuss Isomorphism Theorems}

\begin{definition}
  An element $x$ of a ring $R$ is said to be a \emph{zero-divisor} if there exists an element $y \in R$ such that $xy = 0$. If $R$ is a commutative ring, then in this case, $y$ is also a zero-divisor.
\end{definition}

\begin{definition}
  A ring with no zero divisors is called an \emph{integral domain}.
\end{definition}

\begin{definition}
  An integral domain in which every ideal is principal is called a \emph{principal ideal domain}.
\end{definition}

\begin{definition}
  An integral domain $R$ is said to be a \emph{Euclidean Domain} if there is a function $\sigma: R \to \mathbb{Q}$, called the \emph{size function}, such that...
  \todo{Include properties of size function}
\end{definition}

\subsubsection{Polynomial Rings}

\begin{definition}
  Let $k$ be a field, and let $x$ be an indeterminant. A \emph{polynomial} $f$ is an expression of the form
  \begin{equation*}
  f = \sum_{i = 0}^\infty a_ix^i
  \end{equation*}
  where $a_i \in k$. In this case, $a_i$ are said to be the \emph{coefficients} of $f$. The term containing the highest power of $x$ with non-zero coefficient, is said to be the \emph{leading term}, and the \emph{degree} of $f$ is the power of $x$ in the leading term. Similarly, we define the \emph{leading coefficient} of $f$ to be the coefficient of the leading term.
\end{definition}

We can define several operations on polynomials. Given two polynomials $f = \sum_{i = 0}^\infty a_ix^i, g = \sum_{i = 0}^\infty b_ix^i$ we define their sum $f + g$ to be the polynomial whose coefficients are the sum of the corresponding coefficients of $f$ and $g$. That is,
\begin{equation*}
  f + g = \sum_{i = 0}^\infty (a_i + b_i)x^i
\end{equation*}
The inverse of $f + g$ is denoted $f - g$, wherein the coefficients of $g$ are taken to be their inverses in $k$. Similarly, we define the product of two polynomials $f, g$ to be the expression
\begin{equation*}
  f * g = \sum_{i = 0}^\infty \sum_{j = k = 0}^{j + k = i} a_jb_k x^i
\end{equation*}

\begin{proposition}
  Given a field $k$, $k[x]$, the set of all single variable polynomials with coefficients in $k$ is a commutative ring.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{proposition}
  The set $k[x, \ldots, x_n]$ of all multivariable polynomials with coefficients in $k$ is also a commutative ring.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{definition}
  Let $R[X]$ be as above, and let $f_1, \ldots, f_n$ be polynomials in $R[X]$. We define
  \begin{equation*}
    \langle f_1, \ldots f_n \rangle = \left\{ \sum_{i = 1}^n h_i f_i \mid h_1, \ldots, h_n \in R[X] \right\}
  \end{equation*}
\end{definition}

Given polynomials $f_1, \ldots, f_n$, it is easy to see that $\langle f_1, \ldots, f_n \rangle$ is an ideal. Indeed, $h_1 = h_2 = \cdots = h_n = 0$, then $\sum h_i f_i = 0$, so $0 \in \langle f_1, \ldots, f_n \rangle$. For $f_j, f_k \in \{ f_1, \ldots, f_n \}$, set $h_i$ equal to $1$ if $i = j$ or $k$, and $0$ otherwise, so $f_j + f_k \in \langle f_1, \ldots, f_n \rangle$. For $f_k \in \{ f_1, \ldots, f_n \}$, set $h_i = c$ if $i = k$, and $0$ otherwise, so $c f_k \in \langle f_1, \ldots, f_n \rangle$. So, $\langle f_1, \ldots, f_n \rangle$ is an ideal in $R[X]$, namely, the \emph{ideal generated by $f_1, \ldots, f_n$}.

\begin{proposition}
  Let $k$ be a field, then the polynomial ring of a single variable over $k$, $k[x]$ is a Euclidean domain.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

Note that a polynomial ring of several variables is not a Euclidean domain.

\begin{example}
  \leavevmode \\
  \todo[inline]{Show that $k[x_1, \ldots, x_n]$ is not a Euclidean domain}
\end{example}

\subsection{Modules}

\label{sec:modules}

\begin{definition}
  Let $R$ be a ring. An \emph{$R$-module} $V$ is an abelian group with a law of composition written $+$, and a scalar multiplication $R \times V \to V$, written $r, v \mapsto rv$, such that
  \begin{equation}
  \label{eq:module-definition}
    1v = v \text{, } (rs)v = r(sv) \text{, } (r + s)v = rv + sv, \text{ and } r(v + v') = rv + rv'
  \end{equation}
  for all $r, s \in R, v, v' \in V$.
\end{definition}

\begin{example}
  Let $R$ be a ring. Then, $R$ is also an $R$-module, i.e.
  \begin{align*}
    a + b &= (a + b) * 1 \text{ and ,} \\
    ab &= (ab) * 1 \text{, }
  \end{align*}
  and so on.
\end{example}

\begin{definition}
  Let $V$ be an $R$-module, and let $H$ be a subgroup of $V$ that is also closed under scalar multiplication. In this case, we say that $H$, together with the laws of composition of $V$, is a \emph{submodule} of $V$.
\end{definition}

\todo[inline]{Discuss Isomorphism Theorems}

\begin{example}
  Let $R$ be a ring, and let $I \subset R$ be an ideal of $R$.

  \todo[inline]{Show that $I$ is a submodule of the $R$-module $R$}
\end{example}

\begin{definition}
  Let $V$ be a $R$-module. An ordered set $B = (v_1, \ldots, v_k)$, where $v_i \in V$ is said to generate $V$ if every element $v \in V$ can be written as a linear combination of the elements of B, i.e.
  \begin{equation*}
    v = r_1v_1 + \cdots + r_kv_k
  \end{equation*}
  where $r_i \in R$. Moreover, $V$ is said to be \emph{finitely generated} if there exists a finite set of generators.
\end{definition}

\todo[inline]{Discuss Free Modules}

\begin{proposition}[Ascending Chain Condition]
  \label{prop:acc}
  Let $M$ be an $R$-module, and let $\mathcal{M}$ be the set of submodules of $M$ partially ordered by inclusion (i.e. $M_i \subseteq M_j \land M_j \subseteq M_i \iff M_i = M_j$). Then, the following are equivalent
  \begin{enumerate}[i]
  \item \label{prop:acc-stationary} Every increasing sequence $M_1 \subseteq M_2 \subseteq \cdots \in \mathcal{M}$ is stationary (i.e. there exists $n$ such that $M_n = M_{n + 1} = \cdots$).
  \item \label{prop:acc-maximal} Every non-empty subset of $\mathcal{M}$ has a maximal element.
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{itemize}
  \item \cref{prop:acc-stationary} $\implies$ \cref{prop:acc-maximal}
    If \cref{prop:acc-maximal} is false, then there is a non-empty subset $\mathcal{M'}$ of $\mathcal{M}$ with no maximal element. In this case, $\mathcal{M'}$ is a non-terminating, increasing sequence according to the relation $\subseteq$, which is a contradiction.
  \item \cref{prop:acc-maximal} $\implies$ \cref{prop:acc-stationary}
    Let $\mathcal{M'} \subset \mathcal{M}$ be nonempty. By \cref{prop:acc-stationary} $\mathcal{M'}$ terminates, at say, $M_n$, therefore $\mathcal{M'}$ has maximal element $M_n$.
  \end{itemize}
\end{proof}

A module $M$ satisfying \cref{prop:acc} is said to be \emph{Noetherian}. By \emph{Notherian ring}, we mean a ring $R$ that is Noetherian as an $R$-module.

\begin{proposition}
  Let $M$ be a Notherian $R$-module. Then, every submodule of $M$ is finitely generated.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{theorem}[Hilbert Basis Theorem]
  \label{thm:hilbert-basis}
  Let $R$ be a Noetherian ring. The polynomial ring $R[x]$ is Noetherian.
\end{theorem}

\begin{proof}
  TODO
\end{proof}

Note that by \cref{thm:hilbert-basis} a polynomial ring $k[x_1, \ldots, x_n]$ over a field $k$ is Noetherian.

\subsubsection{Gr\"oebner Bases}

\leavevmode \\

\todo[inline]{Discuss monomial orderings}

\todo{Paraphrase this - copied from Cox, Little \& O'Shea}
\begin{definition}
  Fix a monomial order. A finite subset $G = \{ g_1, \ldots, g_t \}$ of an ideal $I$ is said to be a \emph{Gr\"oebner basis} if
  \begin{equation*}
    \langle \mathrm{LT}(g_1), \ldots, \mathrm{LT(g_n)} \rangle = \langle \mathrm{LT}(I) \rangle
  \end{equation*}
\end{definition}

\begin{proposition}
  Let $k$ be a field, and let $k[x_1, \ldots, x_n]$ be the polynomial ring of $n$ variables over $k$ with a given monomial ordering. Then every ideal $I \subset k[x_1, \ldots, x_n]$, other than $\{ 0 \}$, has a Gr\"oebner basis $G$. Moreover, $G$ is a basis for $I$.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\todo[inline]{Discuss reduced Groebner bases, including uniqueness}

\begin{proposition}
  Let $G$ be a Gr\"oebner basis for $I \subset k[x_1, \ldots, x_n]$, where $k$ is a field, $G^R$, the reduced Gr\"oebner basis of $G$, can be computed as follows:
  \begin{enumerate}
    \item \todo[inline]{Show steps to compute reduced Gr\"oebner basis of $G$}
  \end{enumerate}
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{corollary}
  Given a Gr\"oebner basis $G$ of size $n$, the computation to produce the reduced Gr\"oebner basis $G^R$ of $G$ is $\mathcal{O}(n)$. \todo{Find my notes discussing this}
\end{corollary}

\subsubsection{Matrix Representation of a Linear Operator on a Module}

In this section, let $R = k[x_1, \ldots, x_n]$ be the polynomial ring in $n$ variables over a field $k$, and let $M, N$ be an $R$-modules.

Similar to our discussion on Linear Operators on Vector Spaces (see \cref{sec:preliminaries}), given a linear operator $T: M \to N$, we can produce a \emph{matrix representation} of $T$. Given a basis $\{ e_i \}$ of $M$, i.e. the canonical basis $(0, \ldots, 1, \ldots, 0)$, arrange the mapping of $Te_i$ into a matrix, with $Te_i$ in the $i$th column.

\begin{figure}[h]
  \todo[inline]{Construct matrix representation of $T$, given basis $\{ e_i \}$}
\end{figure}