\documentclass{amsart}

\usepackage{environ}
\usepackage{xparse}

\usepackage{subcaption}

\usepackage{amssymb}

\usepackage[utf8]{inputenc}

\usepackage[%
  backend=biber,%
  natbib=true%
]{biblatex}

\bibliography{thesis}

\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\usepackage[shadow]{todonotes}

\usepackage{hyperref}
\usepackage[german,english]{babel}

\usepackage{listings}
\renewcommand{\lstlistingname}{Algorithm}

\usepackage[shortlabels]{enumitem}

\usepackage{csquotes}
\setquotestyle{american}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[section]{Exercise}
\newtheorem*{ans}{Answer}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator{\LCM}{LCM}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\Fitt}{Fitt}

\numberwithin{equation}{section}

\usepackage{cleveref}
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}

\usepackage{tikz-cd}
\usetikzlibrary{decorations.pathmorphing}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\DeclareDocumentCommand \intodo { o m }
{
  \leavevmode\\
  \IfNoValueTF {#1} {
    \todo[inline]{#2}
  }{
    \todo[inline, #1]{#2}
  }
}

\NewEnviron{aroundtodo}[1][]{
  \intodo[#1]{\BODY}
}

\begin{document}

\title{On the rank of a polynomial matrix}
\author{Patrick McLaren}
\email{patrick.mclaren001@umb.edu}

\begin{abstract}
  We show that performing Gaussian elimination on a polynomial matrix defines rank conditions.
\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

Given a polynomial matrix, that is, a matrix with polynomial entries, what can we say about its rank? If we were to evaluate the entries of the matrix at a point $P$, then we obtain a matrix with entries from a field. In this case, our matrix will have rank $r$ if it has a non-vanishing $r$-minor, whilst all $(r+1)$-minors vanish. However, the rank of our matrix at a point $P$ may not be the same for a different point $Q$.

\intodo[prepend, caption={\textbf{Warning}}]{Examples may be wrong!}

Consider the matrix given in \cref{eq:introduction-linear-matrix}. At $P = (2, 3)$, this matrix will have rank $0$; along the lines $(2, c_1 \not= 3)$ and $(c_2 \not= 2, 3)$, the matrix will have rank $1$;  for all other points, in the quadrants defined by these lines, the matrix will have full rank.
\begin{equation}
  \label{eq:introduction-linear-matrix}
  M_1 =
  \begin{bmatrix}
    x - 2 & 0\\
    0 & y - 3
  \end{bmatrix}
\end{equation}

It would be unreasonable to restrict our attention to idealized matrices of the form of \cref{eq:introduction-linear-matrix}. Our method of taking combinations of diagonal entries fails for all non-diagonal matrices. \Cref{eq:introduction-quadratic-matrix} considers a slightly less contrived example.
\begin{equation}
  \label{eq:introduction-quadratic-matrix}
  M_2 =
  \begin{bmatrix}
    (x - 1)^2 & (y - 1)^2\\
    (x - 2)^2 & (y - 2)^2
  \end{bmatrix}
\end{equation}

In this case, following our method from the first example, $M_2$ will have ranks given by the following coordinates
\begin{align*}
  \rank M(P) = 0 \iff P \in &\, (1, c) \cap (2, c) \cap (c, 1) \cap (c, 2) = \emptyset\\
  \rank M(P) = 1 \iff P \in &\, (1, c) \setminus ((2, c) \cap (c, 1) \cap (c, 2)) \, \bigcup \\
  &\, (2, c) \setminus ((1, c) \cap (c, 1), \cap (c, 2)) \, \bigcup\\
  &\, \cdots\\
  \rank M(P) = 2 \iff P \in &\, \mathbb{R}^2 \setminus \{ (1, 1), (1, 2), (2, 1), (2, 2) \}
\end{align*}

If you look closely at the points excluded as the rank $r$ increases, you'll notice the $r + 1$-minors appearing. At this point, our notation hides the details. Let us consider a generic matrix, given by
\begin{equation}
  \label{eq:introduction-generic-matrix}
  M_3 = \begin{bmatrix}
    f_1 & f_2\\
    f_3 & f_4
  \end{bmatrix}
  \text{, } f_i \in k[x_1, \ldots, x_n]
\end{equation}

Now, before we continue our method, define $Z$ to be the function that takes a set of polynomials to their common zeros, i.e. $Z(\{ x - 1, y - 2 \}) = (1, 2)$. Then, the ranks of $M$ are given by
\begin{align*}
  \rank M(P) = 2 \iff P \in &\, \mathbb{R}^2 \setminus \left( Z(f_1) \cup Z(f_2) \cup Z(f_3) \cup Z(f_4) \right)\\
  \rank M(P) = 1 \iff P \in &\,  \left( Z(f_2) \cup Z(f_3) \cup Z(f_4) \right) \setminus Z(f_1) \, \bigcup \\
  &\, \left( Z(f_1) \cup Z(f_3) \cup Z(f_4) \right) \setminus Z(f_2) \, \bigcup\\
  &\, \left( Z(f_1) \cup Z(f_2) \cup Z(f_4) \right) \setminus Z(f_3) \, \bigcup\\
  &\, \left( Z(f_1) \cup Z(f_2) \cup Z(f_3) \right) \setminus Z(f_4)\\
  \rank M(P) = 0 \iff P \in &\, Z(f_1, f_2, f_3, f_4)
\end{align*}

We shall now relate these algebraic sets for rank $r$ to the minors of a matrix.
\begin{align}
  \rank M(P) = 2 \iff P \in &\, \mathbb{R}^2 \setminus Z(f_1f_4 - f_3f_2)\\
  \rank M(P) = 1 \iff P \in &\, Z(f_1f_4 - f_3f_2) \setminus Z(f_1 , f_2, f_3, f_4)\\
  \rank M(P) = 0 \iff P \in &\, Z(f_1, f_2, f_3, f_4)
\end{align}

More generally, we can easily describe the loci for which an $m \times m$ matrix has rank $r$. Let $\mathcal{D}_{M,r}$ denote the ideal generated by all $r$-minors of a matrix $M$. Then, we have
\begin{align*}
  \rank M(P) = m &\iff P \in \mathbb{A}^m \setminus Z(\mathcal{D}_{M,m})\\
  \rank M(P) = m-1 &\iff P \in Z(\mathcal{D}_{M,m}) \setminus Z(\mathcal{D}_{M,m-1})\\
  \vdots &\phantom{\iff} \vdots\\
  \rank M(P) = 1 &\iff P \in Z(\mathcal{D}_{M,2}) \setminus Z(\mathcal{D}_{M,1})\\
  \rank M(P) = 0 &\iff P \in Z(\mathcal{D}_{M,1})
\end{align*}

Many questions arise if we try to interpret these results in a wider context. For instance,
\begin{enumerate}[(a)]
  \item What can we say, in general, about an $m \times m$ matrix $M$ with entries $m_{i,j}$ in the polynomial ring $k[x_1, \ldots, x_n]$ over a field $k$?
  \item How is the loci of points for which $M$ has rank $r$ related to the space within which $M$ resides
  \item Given a non-generic matrix, is there a general procedure which will produce these points?
\end{enumerate}
\begin{aroundtodo}[prepend, caption={Summary of Questions}]
  \leavevmode \\
  \begin{enumerate}[(a)]
    \item An $m \times m$ matrix $\varphi^{*}$ with entries $m_{i,j} \in R = k[x_1, \dots, x_n]$ corresponds to a finitely presented $R$-module $M$.
    \item The closed subset of $\Spec R$ defined by $\Fitt_j M$ is the set of primes $Q$ such that $M_Q$ cannot be generated by $j$ elements
    \item By performing Gaussian elimination, we can find sub-ideals corresponding to sub-varieties of the rank loci
  \end{enumerate}
\end{aroundtodo}

%\begin{itemize}
%\item Fulton related the loci associated with a partial permutation to Schubert polynomials in \cite{fulton1992}
%\item For an introductory view to the subject of Schubert varieties and determinantal ideals, see the books by Sturmfels and Miller \cite{miller2005combinatorial}, and Fulton \cite{fulton1998schubert}.
%\end{itemize}

\section{Modules}

\subsection{Preliminaries}

Pedagogically, matrices typically in the study of Linear Algebra, as representations of linear maps between vector spaces. Polynomial matrices, however, are representations of maps between certain spaces over polynomial rings. More specifially, they are representations of maps between certain \emph{modules}.

\begin{definition}
  Let $R$ be a commutative ring. An \emph{$R$-module} is an abelian group $M$ together with a bilinear map $R \times M \to M$, written $(r, m) = rm$ for $r \in R, m \in M$, that satisfies
  \begin{enumerate}[(a)]
  \item Associativity: $r(sm) = (rs)m$
  \item Bilinearity:
    \begin{enumerate}[(i)]
    \item $(r + s)m = rm + sm$
    \item $r(m + n) = rm + rn$
    \end{enumerate}
  \item Identity: $1m = m$
  \end{enumerate}
  for all $r, s \in R, m, n \in M$.
\end{definition}

\begin{example}
  \begin{aroundtodo}
    Show that any ring $R$ is a $\mathbb{Z}$-module.
  \end{aroundtodo}
\end{example}

The definition of a module is suggestive of vector spaces, however, with one particular difference, that being bases. We shall soon see exactly which modules have bases.

First, suppose we have two $R$-modules, $M, N$. There is a basic pairing of $M$ and $N$, which is also an $R$-module. Given two groups $G, G'$, their direct sum, $G \oplus G'$, is also a group. Similarly, we can define the direct sum for modules.

\begin{definition}
  Let $M, N$ be $R$-modules, then the \emph{direct sum} of $M, N$, denoted $M \oplus N$, where
  \begin{equation*}
    M \oplus N = \{ (m, n) \mid m \in M, n \in N \}
  \end{equation*}
  This construction is also an $R$-module, given by the map $R \times (M \oplus N) \to M \oplus N$, where $r(m, n) \mapsto (rm, rn)$ for $r \in R, (m, n) \in M \oplus N$. A notable example of the direct sum is $\oplus^n R$, where $R$ is a ring, which is said to be a \emph{free module}, of \emph{rank} $n$.
\end{definition}

\begin{definition}
  An $R$-module $M$ is said to be \emph{finitely generated} if there is a finite set of elements that generate $M$.
\end{definition}

Recall that a ring $R$ has the \emph{invariant basis number} property if all finitely generated free $R$-modules have a rank.

\begin{example}
  Let $R = k[x_1, \dots, x_n]$ be a polynomial ring over a field $k$ in $n$ variables. Let $M = \oplus^m R$, that is, the direct sum of $m$ copies of $R$. Then, $M$ is a free module of rank $m$.
\end{example}

Recall that a ring $R$ is said to be \emph{Noetherian} if every ideal of $R$ is finitely generated.

\begin{theorem}[Hilbert Basis Theorem]
  Let $R$ be a Noetherian ring, then $R[x]$, the ring of polynomials of single variable over $R$ is Noetherian.
\end{theorem}

\begin{proof}
  TODO
\end{proof}

\begin{proposition}
  Let $R$ be a Noetherian ring, and $M$ a finitely generated $R$-module. Then $M$ is Noetherian.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{definition}
  An $R$-module $P$ is projective if for every epimorphism of modules $\alpha: M \to N$ and every map $\beta: P \to N$, there exists a map $\gamma: P \to M$ such that $\beta = \alpha \gamma$, as in the following figure
  \begin{equation*}
    \begin{tikzcd}
      & P \arrow[dl, dashed]{}{\exists \gamma} \arrow[d]{}{\beta} \\
      M \arrow[r]{}{\alpha} & N
    \end{tikzcd}
  \end{equation*}
  \intodo{Taken straight from \cite{MR1322960}, need to make digestable}
\end{definition}

\subsection{Exact Sequences}

\begin{definition}
  Let $(F_i){i \in \Lambda}$ be $R$-modules, and let $\varphi_i: F_{i} \to F_{i-1}$ be a sequence of $R$-module homomorphisms. The sequence
  \begin{equation*}
    \begin{tikzcd}
      \cdots \arrow[r]{}{\varphi_{i+2}} & F_{i+1} \arrow[r]{}{\varphi_{i+1}} & F_{i} \arrow[r]{}{\varphi_{i}} & F_{i-1} \arrow[r]{}{\varphi_{i-1}} & \cdots
    \end{tikzcd}
  \end{equation*}
  is said to be \emph{exact} if $\Ima \varphi_{i} = \ker \varphi_{i-1}$, for all $i \in \Lambda$.

  Given, $R$-modules $A$, $B$, and $C$, a \emph{short exact sequence} is an exact sequence
  \begin{equation*}
    \begin{tikzcd}
      0 \arrow[r] & A \arrow[r]{}{\alpha} & B \arrow[r]{}{\beta} & C \arrow[r] & 0
    \end{tikzcd}
  \end{equation*}
  In this case, $\alpha$ is injective, and $\beta$ is surjective.
  \intodo{Check \cite{MR1322960} for correctness; define homology}
\end{definition}

\begin{definition}
  Let $M$ be an $R$-module. A \emph{free resolution} of $M$ is an exact sequence of free modules $(F_{i}, \varphi_{i})$, such that $M = \coker \varphi_{1}$. That is,
  \begin{equation*}
    \begin{tikzcd}
      \cdots \arrow[r]{}{\varphi_{2}} & F_1 \arrow[r]{}{\varphi_{1}} & F_0 \arrow[r] & 0 
    \end{tikzcd}
  \end{equation*}
\end{definition}

\begin{definition}
  Let $M$ be an $R$-module. $M$ is said to be \emph{finitely presented} if there exist finitely generated free $R$-modules $F, G$ such that the following sequence
  \begin{equation*}
    \begin{tikzcd}
      F \arrow[r] & G \arrow[r] & M \arrow[r] & 0
    \end{tikzcd}
  \end{equation*}
  is exact.
\end{definition}

\subsection{Presentation Matrix of a Module}

Let $R = k[x_1, \ldots, x_n]$ be a polynomial ring over a field $k$, and let $\varphi^{*}$ be an $m \times n$ matrix with entries in $R$. Let $M = \Ima \varphi^{*}$.

\begin{proposition}
  The set $M$, given above, is an $R$-module. Moreover, $M$ is finitely presented, with presentation matrix $\varphi^{*}$.
\end{proposition}

\begin{proof}
  Given free modules $R^m$, $R^n$, and an appropriate basis $(\mathbf{e}_i)_{1 \leq i \leq m}$ for $R^m$, define $\varphi: R^n \to R^m$ by
  \begin{equation*}
    \varphi(e_i) = \varphi^{*}_i \text{, for } 1 \leq i \leq m,
  \end{equation*}
  where $\varphi^{*}_i$ is the $i$th column of $\varphi^{*}$. In this case, $\varphi$ is an $R$-module homomorphism, indeed, we have $\varphi(\alpha r) = \alpha \varphi(r)$ and $\varphi(r + r') = \varphi(r) + \varphi(r')$ due to the assocativity and distributivity of the base ring $R$. That $\Ima \varphi = \Ima \varphi^{*} = M$ can be seen by noting our choice of basis for $R^m$. So, we have the following sequence
  \begin{equation*}
    \begin{tikzcd}
      R^m \arrow[r]{}{\varphi} & R^n \arrow[r] & M \arrow[r] & 0
    \end{tikzcd}
  \end{equation*}
  which is exact. \todo{Follow \cite{MR1322960}, pg. 17, Example 3, to show exactness} Therefore, since $R^m, R^n$ are finitely generated free modules over $R$, $M$ is finitely presented.
\end{proof}

\subsection{Finite Projective Modules over Polynomial Rings}

\begin{theorem}[Serre]
  If $k$ is a field and $x_1, \dots, x_n$ are independent variables, then every finite projective module over $k[x_1, \dots, x_n]$ is stably free, or equivalently admits a finite free resolution. \todo{Taken straight from \cite{MR1878556}, paraphrase}
\end{theorem}

\begin{proof}
  TODO
\end{proof}

\todo[inline]{Define unimodular}

\begin{theorem}[Quillen-Suslin]
  Let $k$ be a field and let $f$ be a unimodular vector in $k[x_1, \dots, x_r]^{(n)}$. Then $f$ has the unimodular extension property. \todo{Taken straight from \cite{MR1878556}, paraphrase}
\end{theorem}

\begin{proof}
  TODO
\end{proof}

\begin{theorem}
  Let $A$ be a commutative ring which has the unimodular column extension property. Then every stably free module over $A$ is free. \todo{Taken straight from \cite{MR1878556}, paraphrase}
\end{theorem}

\begin{proof}
  TODO
\end{proof}

\begin{theorem}
  Let $k$ be a field. Then every finite projective module over the polynomial ring $k[x_1, \dots, x_n]$ is free.
\end{theorem}

\begin{proof}
  TODO
\end{proof}

By the Quillen-Suslin theorem, every finitely generated projective module over a polynomial ring is free. So, if $A, B$ are finitely generated projective $R$-modules, and $f: A \to B$ is an epimorphism, then given a basis $E$ on $A$, we can find a matrix representation of $f$, say $M_B$, and examine its degeneracy loci. In this section, we shall discuss degeneracy loci from the point of view of $f$, an $R$-module homomorphism.

\begin{aroundtodo}[prepend, caption={\textbf{Todo}}]
  \begin{itemize}
  \item The category of affine algebraic sets and morphisms (over an algebraically closed field $k$) is equivalent to the category of affine $k$-algebras with the arrows reversed (see \cite{MR1322960}, Corollary 1.10).
  \end{itemize}
\end{aroundtodo}

\section{Degeneracy Loci and Fitting Ideals}

\subsection{Multilinear Algebra}

Let $A$, $B$, and $C$ be sets, and suppose that $\Delta$ is a bilinear map from $A$ and $B$ to $C$. Then, $\Delta$ satisfies the bilinearity property, i.e.,
\begin{equation*}
  \Delta(\alpha a + \alpha' a', \beta b + \beta' b') = \alpha \beta \Delta(a, b) + \alpha' \beta \Delta(a', b) + \alpha \beta' \Delta(a, b') + \alpha \beta \Delta(a', b')
\end{equation*}
for $\alpha, \alpha', a, a \in A, \beta, \beta', b, b' \in B$. If we want to capture this behavior in a more general setting, i.e. without $C$, we can define the \emph{tensor product}, or $A \otimes B$, where
\begin{equation*}
  A \otimes B = \{ a \otimes b \mid a \in A, b \in B \}
\end{equation*}
with the relation of bilinearity around $\otimes$, i.e.,
\begin{equation*}
  (\alpha a + \alpha' a') \otimes (\beta b + \beta' b') = \alpha \beta (a \otimes b) + \alpha' \beta (a' \otimes b) + \alpha \beta' (a \otimes b') + \alpha \beta (a' \otimes b')
\end{equation*}
for $\alpha, \alpha', a, a \in A, \beta, \beta', b, b' \in B$. In our case, if $M, N$ are $R$ modules, then $M \otimes N$ is also an $R$-module, given by the map $r (m \otimes n) = rm \otimes rn$. In this case, we write $M \otimes_R N$ to indicate the $R$-module structure.

There is a natural map from $M$ to $M \otimes_R N$, given by $m \mapsto m \otimes 1$, and similarly for $N$. Then, given any $R$-module homomorphism map $\gamma$, from $M \times N$ to $A$, there is a unique map $\overline{\gamma}$, from $M \otimes_R N$ to $A$, satisfying $\overline{\gamma}(a \otimes b) = \gamma(a, b)$, by $m \otimes n \mapsto \gamma(m, n)$.

\intodo{Introduce more multilinear algebra}

\subsection{Fitting Ideals}

\begin{definition}
  Let $F, G$ be free modules, and let $\varphi: F \to G$, let $I_j\varphi$ be the image of the map
  \begin{equation*}
    \wedge^j F \otimes \wedge^j G^* \to R
  \end{equation*}
  induced by $\wedge^j \varphi: \varphi^j F \to \varphi^j G$. \todo{Almost straight from \cite{MR1322960}, paraphase more}
\end{definition}

Given bases for $F$ and $G$, we may represent $\varphi$ by a matrix. In this case, $I_j \varphi$ is generated by the $j$-minors of the matrix. \todo{Elaborate more!}

\begin{definition}
  Let $M$ be a finitely generated $R$-module, and let $\varphi: F \to G \to M \to 0$ be a presentation of $M$. The $i$th \emph{Fitting ideal} of $M$, $\Fitt_i(M)$, is the ideal
  \begin{equation*}
    \Fitt_i(M) = I_{r-i}\varphi
  \end{equation*}
\end{definition}

\begin{proposition}[Fitting's Lemma]
  Let $M$ be a finitely generated $R$-module, and let $\varphi: F \to G \to M \to 0$, and $\varphi: F' \to G' \to M \to 0$, where $G$ and $G'$ are finitely generated free modules of rank $r$ and $r'$. Then, $I_{r-i}(\varphi) = I_{r'-i}(\varphi')$,
  for all $0 \leq i < \infty$.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

We shall now refer to the $i$th Fitting ideal as the $i$th Fitting \emph{invariant}.

\begin{proposition}
  If $(R, P)$ is local, then $M$ can be generated by $j$ elements iff $\Fitt_j M = R$. In general, the closed subset of $\Spec R$ defined by $\Fitt_j M$ is the set of primes $Q$ such that $M_Q$ cannot be generated by $j$ elements.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\subsection{Permutations, Schubert Polynomials, and Schubert Varieties}

Let $\mathcal{M}_{k \times l}$ denote the vector space of matrices with $k$ rows and $l$ columns.

\begin{definition}
  Let $w$ be a \emph{partial permutation}. The \emph{matrix Schubert variety} $\overline{X}_w$ inside $\mathcal{M}_{k \times l}$ is the subvariety
  \begin{equation*}
    \overline{X}_w = \{ Z \in \mathcal{M}_{k \times l} \mid \rank(Z_{p \times q}) \leq \rank(w_{p \times q}) \text{ fror all $p$ and $q$} \}
  \end{equation*}
  Also, let $r(w)$ be the $k \times l$ \emph{rank array} whose entry at $(p, q)$ is $r_{pq}(w) = \rank(w_{p \times q})$.
\end{definition}

\begin{definition}
  Let $w \in \mathcal{M}_{k \times l}$ be a partial permutation. The \emph{Schubert determinantal ideal} $I_w \subset k[\mathbf{x}]$ is generated by all minors $\mathbf{x}_{p \times q}$ of size $1 + r_{pq}(w)$ for all $p$ and $q$, where $\mathbf{x} = (x_{\alpha\beta})$ is the $k \times l$ matrix of variables.
\end{definition}

\begin{proposition}
  Every partial permutation matrix $w$ can be extended canonically to a square permutation matrix $\overset{\sim}{w}$ whose Schubert determinantal ideal $I_{\overline{w}}$ has the same minimal generating minors as $I_{\overline{w}}$.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{proposition}
  Let $V_r \subset k^n$ such that $\rank(A(s)) = r$ for all $s \in V_r$. Then, the degree of $V_r$ is given by
  \begin{equation*}
    \mathfrak{S}_w(p_1, p_2, \ldots, p_l, 0, \ldots, -q_1, -q_2, \ldots, -q_m, 0, \ldots, 0)
  \end{equation*}
  where $\mathfrak{S}_w(x_1, \ldots, x_n, y_1, \ldots, y_n)$ is the double Schubert polynomial, and $w$ is the associated permutation for $A$ under $V_r$.
\end{proposition}

\begin{proof}
  \intodo{Apply proof from \cite{fulton1992}}
\end{proof}

% Use the natural map $k[\mathbf{x}] \to (k[\mathbf{x}]/I(\overline{X}_{\sigma_iw}))_{\mathfrak{m}}$ from Lemma 15.36 \cite{miller2005combinatorial} (page 303) to connect

\section{Gaussian Elimination}

Recall that performing Gaussian elimination allows us to solve the equation $AX = B$ by producing matrices $P, L, U$ such that $A = PLU$, where $P$ is a permutation matrix, and $L, U$ are (respectively) lower, and upper triangular matrices. The solution to the equation $AX = B$ is then found by back-substitution.

However, given indeterminate entries $a_{i,j} \in k[\mathbf{x}]$, a decision must be made as to which entries are to be considered zero. Let $I(A, S)$ denote the entries of $A$ which are equal to zero on $S \subset k^n$, where
\begin{equation*}
  I(A, S) = \{ a_{i,j} \in A \mid a_{i,j} \in I(S), S \subset k^n \}
\end{equation*}
Now, let $\pi$ denote the map that takes $A \mapsto A'$, by $a_{i,j} \mapsto \overline{a_{i,j}} \in k[\mathbf{x}]/I(A, S)$.
\begin{aroundtodo}[caption={}]
  Why care about $I(A, S)$, why not just use $I(S)$? Then, we can just use $a_{i,j} \mapsto \overline{a_{i,j}} \in k[x_1, \ldots, x_n]/I(S)$, where
  \begin{equation*}
    k[x_1, \ldots, x_n]/I(S) \eqsim \mathcal{O}(S)
  \end{equation*}
\end{aroundtodo}

\begin{definition}
  Let $*$ be the function that sends a polynomial matrix to a matrix with entries in a field by evaluating it's entries, i.e. $a_{i,j} \mapsto a_{i,j}(s)$. We'll usually write $A(s)$ to mean the same.
\end{definition}

\begin{proposition}
  Given a subset $S \subset k^n$, the $LU$ factorization of $A(S)$ with pivoting, produces the same factorization as $\pi(A, S) = A'$.
  \begin{equation*}
    \begin{tikzcd}
      A \arrow[dr]{}{*} \arrow[r]{}{\pi_S} & \arrow[d]{}{*} (PLU)'\\
      & PLU
    \end{tikzcd}
  \end{equation*}
\end{proposition}

\begin{proof}
  \intodo{To show this, we'll need to first define LU factorization modulo an ideal. Then, probably proceed by induction using Gaussian elimination.}
\end{proof}

What can we say about the rank of $A$ on $S \subset k^n$? Let $V_r \subset k^n$ denote the loci for which $A$ has rank at most $r$. Suppose that all minors are homogeneous, or equivalently,
\begin{equation*}
  \mathrm{degree}(a_{i,j}) = p_i + q_j
\end{equation*}
for some integers $p_1, \ldots, p_l, q_1, \ldots, q_m$. \todo{See Proposition 5.15 in \emph{Using Algebraic Geometry}} We can determine the degree of $V_r$. Let $A' = PLU$ be the factorization of $A' = \pi(A, V_r)$.

\begin{definition}
  \begin{aroundtodo}
    Given $V_r$, let $A' = \pi(A, V_r)$. Identify the choice of $r$ pivots used in Gaussian elimination of $A'$. These pivots prescribe rank conditions on $A'$ (this might be a proposition).

    If the permutation matrix $P$ in $A' = PLU$ satisfies these rank conditions, add a lemma to show this, then call $P$ the \emph{associated permutation for $A$ under $V_r$}. Otherwise, the define the \emph{associated permutation for $A$ under $V_r$} as the permutation that corresponds to these rank conditions (see \cite{fulton1998schubert}, page 9).
  \end{aroundtodo}
\end{definition}

% \newpage
%
% \section{Old}
%
% A polynomial matrix is a linear operator on a module over a polynomial ring. The rank of a linear operator on a module is not well defined, however given a polynomial matrix, we can evaluate it's entries on $s \in F$ to produce a matrix $A(s)$ on a vector space. Then, we can proceed to determine the rank of $A(s)$.
%
% The situation is slightly different if one does not immediately wish to obtain $A(s)$. Perhaps we would like to know about the conditions for which the matrix fails to have rank $m$. Suppose that by manipulating $A$, we can produce a row-reduced echelon form of $A$, say $A'$, such that $\mathrm{rank}\,A(s) = \mathrm{rank}\,A'(s)$. Then, one may examine the pivots to determine the rank. However, this leads to several problems:
%
% \begin{enumerate}
% \item As we manipulate the matrix $A$, we may assume that a given entry $(i,i)$ will be non-zero in $A'(s)$, as shown in \cref{fig:pivot-example}.
% \begin{figure}[h]
%   \centering
%   \begin{align*}
%     \begin{bmatrix}
%       f_1 & \dots & \dots & \dots \\
%       \hdotsfor{4} \\
%       f_i & \dots & \dots & \dots \\
%       \hdotsfor{4}
%     \end{bmatrix} & \overset{\text{pivot by } f_1}{\to} \begin{bmatrix}
%       1 & \dots & \dots & \dots \\
%       \hdotsfor{4} \\
%       0 & \dots & \dots & \dots \\
%       \hdotsfor{4}
%     \end{bmatrix}
%   \end{align*}
%   \caption{Assuming $f_1(s) \not= 0$}
%   \label{fig:pivot-example}
% \end{figure}
%
% Rather than selecting pivots which are not equal to the zero-polynomial, our method should consider the possibility that $a_{i,i}(s) = 0$ for some $s \in F$.
%
% \item Pivoting in the traditional manner using Gaussian Eliminiation (i.e. $a_{i,i} \mapsto 1$) produces a block matrix as shown in \cref{fig:gaussian-eliminiation-example}. In this case, by the conclusion of the algorithm, all information regarding the pivots has been lost.
% \begin{figure}[h]
%   \centering
%   \begin{equation*}
%     \left[
%     \begin{array}{c | c c }
%       \raisebox{0pt}{{\large\mbox{{$I$}}}} & \dots & 0 \\ \hline
%       \vdots & \ddots & \vdots \\
%       0 & \dots & 0
%     \end{array}
%     \right]
%   \end{equation*}
%   \caption{Output of Gaussian Elimination}
%   \label{fig:gaussian-eliminiation-example}
% \end{figure}
%
% \item Depending on our choice of pivots, we could potentially produce multiple matrices $A', A'', \ldots$ from $A$ with the property $\mathrm{rank} \, A'(s) = \mathrm{rank} \, A''(s) = \cdots = \mathrm{rank} \, A(s)$. Given a collection of pivots, say $P', P''$, from $A', A''$, we would like some nice way of comparing $P'$ and $P''$.
%
% \end{enumerate}
%
% For every pivot $a_{i,i}$ that we choose to be non-zero, we must also take into account the possibility that $a_{i,i}(s) = 0$ for some $s \in F$, potentially producing $n!$ different matrices.
%
% Every row-reduced matrix $A'$ that we produce has an associated collection of pivots which are assumed to be non-zero $P = (a_{i,i})_{i \in \Lambda}$, and a collection of entries which are assumed to be zero $Q = (a_{i,j})_{i \in \Delta}$. In this case, $q \nmid p$ for all $q \in Q, p \in P$. Equivalently, the elements of $P$ are elements of the quotient ring $F[X]/I$, where $I = \langle Q \rangle$.
%
% A generic matrix $A$ of size $m \times l$ has rank $k$ when there exists at least one non-zero $k$ minor whilst $n_{> k}$ minors vanish. These equations define (cut out) a locus of points. There is a large amount of existing literature investigating these points and the ideals generated by the corresponding minors.
%
% When performing Gaussian Elimination to produce a row-echelon matrix, one must make certain assumptions about the value of pivots. In general, a given elimination path will produce an upper triangular matrix $A'$ with $k$ pivots. Then, if the assumptions made regarding the value of the pivots is correct, the rank of $A'$, and thus $A$, is determined by the number of non-zero pivots.
%
% Given an assumption that $f_1 \not= 0, \ldots, f_k \not= 0$, how do these rank conditions relate to the vanishing $k+1$ minors of $A$?
%
% Note that these points is the intersection of the complement of the varieties of the equations $f_1, \ldots, f_k$.
%
% \begin{itemize}
% \item given that this minor is a polynomial in the entries of a generic matrix, can this minor be described in terms of permutations?
% \item the pivots which show up in my computation correspond to rank conditions of the original matrix, in the form of dot diagrams seen in the literature
% \item thus, a eliminiation path, corresponds to a permutation, and the locus of these points is represented by a double Schubert polynomial (Fulton, page 21, Sturmfels, page 300)
% \item different loci can be determined through different permutations, equivalent to manipulating Schubert polynomials (Fulton, page 22)
% \item so different permutations are equivalent to determining different rank conditions, i.e. branching is due to our desire to determine conditions for rank $k \leq n$
% \item also, different permutations with same rank may provide different loci
% \item is the rank in elimination equal to some number for permutations, schubert polynomials?
% \end{itemize}

\section{Source Code}

Source code available at \url{https://github.com/patrickmclaren/math-thesis}.

\subsection{Code Documentation}

Documentation available at \url{https://patrickmclaren.github.io/polyrank-docs}.

\newpage

%\section{Appendix}
%
%\input{appendix.tex}

\newpage

\printbibliography

\end{document}