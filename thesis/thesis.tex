\nonstopmode

\documentclass{amsart}

\usepackage{amssymb}

\usepackage[utf8]{inputenc}

\usepackage[
  backend=biber,
  natbib=true
]{biblatex}

\bibliography{thesis}

\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\usepackage{todonotes}

\usepackage{hyperref}
\usepackage[german,english]{babel}

\usepackage{listings}
\renewcommand{\lstlistingname}{Algorithm}

\usepackage{enumerate}

\usepackage{cleveref}
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}

\usepackage{csquotes}
\setquotestyle{american}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[section]{Exercise}
\newtheorem*{ans}{Answer}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator{\LCM}{LCM}

\numberwithin{equation}{section}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title Page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\thetitle{On The Rank of a Polynomial Matrix}
\def\theauthor{Patrick McLaren}
\def\theemail{patrick.mclaren001@umb.edu}

\begin{titlepage}
  \begin{center}
    \textsc{\LARGE \thetitle}\\[0.75cm]
    \textsc{By}\\[0.25cm]
    \textsc{\large \theauthor}\\[1cm]
  \end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}

\section*{Abstract}

\begin{center}
  \begin{minipage}{0.8\textwidth}
    \small
    \begin{flushleft}
      The abstract goes here.
    \end{flushleft}
  \end{minipage}
\end{center}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}

\section*{Plan}

\subsection*{Overview}

\begin{enumerate}
\item Statement of problem, motivating example, and outline of methods
\item Recall basic definitions and theorems of linear algebra including Gaussian Elimination
\item Introduce polynomial rings, ideals, and gr\"oebner bases (along with all prerequisites, cf. Hilbert Basis Theorem, noetherian rings, A.C.C)
\item Introduce modules, free modules, finite presentations
\item Discuss row reducing a polynomial matrix (presentation matrix)
\item Discuss conditions a matrix has rank $m \leq n$
\end{enumerate}

\subsection*{Potential References}

I've noted a sharp drop in the number of accessible resources regarding modules over a polynomial ring in several variables, in contrast to the single variable case. Here's a list of references I have recently viewed. I may temporarily block-quote these references for easy viewing in \cref{sec:notes-from-sources}.

\subsubsection*{Module over a Polynomial Ring of Single Variable}

\begin{itemize}
\item \fullcite{artin2014algebra}
\end{itemize}

\subsubsection*{Module over a Polynomial Ring of Several Variables}

\begin{itemize}
\item \fullcite{cox2007ideals}
\end{itemize}

\subsubsection*{Other}

\begin{itemize}
\item \fullcite{bruns}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\setcounter{page}{1}

\section{Introduction}

A polynomial matrix is a linear operator on a module over a polynomial ring. The rank of a linear operator on a module is not well defined, however given a polynomial matrix, we can evaluate it's entries on $s \in F$ to produce a matrix $A(s)$ on a vector space. Then, we can proceed to determine the rank of $A(s)$.

The situation is slightly different if one does not immediately wish to obtain $A(s)$. Perhaps we would like to know about the conditions for which the matrix fails to have rank $m$. Suppose that by manipulating $A$, we can produce a row-reduced echelon form of $A$, say $A'$, such that $\mathrm{rank}\,A(s) = \mathrm{rank}\,A'(s)$. Then, one may examine the pivots to determine the rank. However, this leads to several problems:

\begin{enumerate}
\item As we manipulate the matrix $A$, we may assume that a given entry $(i,i)$ will be non-zero in $A'(s)$, as shown in \cref{fig:pivot-example}.
\begin{figure}[h]
  \centering
  \begin{align*}
    \begin{bmatrix}
      f_1 & \dots & \dots & \dots \\
      \hdotsfor{4} \\
      f_i & \dots & \dots & \dots \\
      \hdotsfor{4}
    \end{bmatrix} & \overset{\text{pivot by } f_1}{\to} \begin{bmatrix}
      1 & \dots & \dots & \dots \\
      \hdotsfor{4} \\
      0 & \dots & \dots & \dots \\
      \hdotsfor{4}
    \end{bmatrix}
  \end{align*}
  \caption{Assuming $f_1(s) \not= 0$}
  \label{fig:pivot-example}
\end{figure}

Rather than selecting pivots which are not equal to the zero-polynomial, our method should consider the possibility that $a_{i,i}(s) = 0$ for some $s \in F$.

\item Pivoting in the traditional manner using Gaussian Eliminiation (i.e. $a_{i,i} \mapsto 1$) produces a block matrix as shown in \cref{fig:gaussian-eliminiation-example}. In this case, by the conclusion of the algorithm, all information regarding the pivots has been lost.
\begin{figure}[h]
  \centering
  \begin{equation*}
    \left[
    \begin{array}{c | c c }
      \raisebox{0pt}{{\large\mbox{{$I$}}}} & \dots & 0 \\ \hline
      \vdots & \ddots & \vdots \\
      0 & \dots & 0
    \end{array}
    \right]
  \end{equation*}
  \caption{Output of Gaussian Elimination}
  \label{fig:gaussian-eliminiation-example}
\end{figure}

\item Depending on our choice of pivots, we could potentially produce multiple matrices $A', A'', \ldots$ from $A$ with the property $\mathrm{rank} \, A'(s) = \mathrm{rank} \, A''(s) = \cdots = \mathrm{rank} \, A(s)$. Given a collection of pivots, say $P', P''$, from $A', A''$, we would like some nice way of comparing $P'$ and $P''$.

\end{enumerate}

\subsection{Outline of Method}

For every pivot $a_{i,i}$ that we choose to be non-zero, we must also take into account the possibility that $a_{i,i}(s) = 0$ for some $s \in F$, potentially producing $n!$ different matrices.

Every row-reduced matrix $A'$ that we produce has an associated collection of pivots which are assumed to be non-zero $P = (a_{i,i})_{i \in \Lambda}$, and a collection of entries which are assumed to be zero $Q = (a_{i,j})_{i \in \Delta}$. In this case, $q \nmid p$ for all $q \in Q, p \in P$. Equivalently, the elements of $P$ are elements of the quotient ring $F[X]/I$, where $I = \langle Q \rangle$.

\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Review of Linear Algebra}

\subsubsection{Vector Spaces}

\begin{definition}
    Let $F$ be a field. Let $V$ be a subset of $F$, and suppose that $V$
    satisfies the following properties
    \begin{enumerate}[i]
        \item $v_1, v_2 \in V \implies v_1 + v_2 \in V$
        \item $v_1 \in V, c \in F \implies c * v_1 \in V$
    \end{enumerate}
    In this case, $V$ is said to be a \emph{vector space} over $F$.
\end{definition}

\subsubsection{Matrix Representation of a Linear Transformation}

\subsubsection{Determinants}

\todo[inline]{Include recursive formula as given in \cite{artin2014algebra}

\subsubsection{Row Echelon Form}

The setting in which one wants to produce an upper triangular matrix usually involves some desire to solve the equation
\begin{equation*}
  AX = B
\end{equation*}
where $A$ is an invertible linear transformation, and $X, B$ are vectors (or elements of a free module). One can perform \emph{Gaussian Elimination} to produce an upper triangular matrix (or \emph{Gauss-Jordan Elimination} to produce the reduced row echelon form of a matrix) by applying a sequence of elementary row operations to the system. \\

Now, these elementary row operations can be represented as elementary matrices, say $E_1, \ldots, E_k$. Applied to the original system, we have
\begin{align*}
  E_k \cdots E_1 AX &= E_k \cdots E_1 B\\
  A'X &= B'
\end{align*}

We know that these operations do not change the solution to the equation by application of the cancellation law, since elementary matrices are invertible. Then, one may obtain $X$ by back-substitution. If $A$ was not invertible, then a pseudoinverse may be obtained to describe all solutions that satisfy the equation.

\section{Rings}
\label{seq:rings}

\begin{definition}
  A \emph{ring} is a set $R$ with two binary operations $+, \times$, called addition and multiplication respectively, that satisfy the following properties:
  \begin{itemize}
  \item $R, +$ is an abelian group
  \item $R, *$ is a group
  \item Distributive property
    \begin{itemize}
    \item Left Distributive: $a(b + c) = ab + ac$
    \item Right Distributive: $(a + b)c = ac + bc$
    \end{itemize}
  \end{itemize}
\end{definition}

A ring in which multiplication is commutative is called a \emph{commutative ring}.

\todo[inline]{Discuss subrings}

\begin{definition}
  An \emph{ideal} $I$ is a subset of a ring $R$ that satisfies the following properties:
  \begin{itemize}
  \item $0 \in I$
  \item If $a, b \in I$, then $a + b \in I$
  \item If $a \in I, c \in R$, then $ca \in I$
  \end{itemize}
\end{definition}

Given an element $a$ of a ring $R$, the set $(a)$ containing all multiples of $a$ is an ideal, i.e. $0*a = 0 \in (a)$, $a + a = 1*a + 1*a = (1 + 1)*a \in (a)$. In this case, $(a)$ is said to be the \emph{principal ideal generated by $a$}.

\todo[inline]{Discuss Isomorphism Theorems}

\begin{definition}
  An element $x$ of a ring $R$ is said to be a \emph{zero-divisor} if there exists an element $y \in R$ such that $xy = 0$. If $R$ is a commutative ring, then in this case, $y$ is also a zero-divisor.
\end{definition}

\begin{definition}
  A ring with no zero divisors is called an \emph{integral domain}.
\end{definition}

\begin{definition}
  An integral domain in which every ideal is principal is called a \emph{principal ideal domain}.
\end{definition}

\begin{definition}
  An integral domain $R$ is said to be a \emph{Euclidean Domain} if there is a function $\sigma: R \to \mathbb{Q}$, called the \emph{size function}, such that...
  \todo{Include properties of size function}
\end{definition}

\subsection{Polynomial Rings}

\begin{definition}
  Let $k$ be a field, and let $x$ be an indeterminant. A \emph{polynomial} $f$ is an expression of the form
  \begin{equation*}
  f = \sum_{i = 0}^\infty a_ix^i
  \end{equation*}
  where $a_i \in k$. In this case, $a_i$ are said to be the \emph{coefficients} of $f$. The term containing the highest power of $x$ with non-zero coefficient, is said to be the \emph{leading term}, and the \emph{degree} of $f$ is the power of $x$ in the leading term. Similarly, we define the \emph{leading coefficient} of $f$ to be the coefficient of the leading term.
\end{definition}

We can define several operations on polynomials. Given two polynomials $f = \sum_{i = 0}^\infty a_ix^i, g = \sum_{i = 0}^\infty b_ix^i$ we define their sum $f + g$ to be the polynomial whose coefficients are the sum of the corresponding coefficients of $f$ and $g$. That is,
\begin{equation*}
  f + g = \sum_{i = 0}^\infty (a_i + b_i)x^i
\end{equation*}
The inverse of $f + g$ is denoted $f - g$, wherein the coefficients of $g$ are taken to be their inverses in $k$. Similarly, we define the product of two polynomials $f, g$ to be the expression
\begin{equation*}
  f * g = \sum_{i = 0}^\infty \sum_{j = k = 0}^{j + k = i} a_jb_k x^i
\end{equation*}

\begin{proposition}
  Given a field $k$, $k[x]$, the set of all single variable polynomials with coefficients in $k$ is a commutative ring.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{proposition}
  The set $k[x, \ldots, x_n]$ of all multivariable polynomials with coefficients in $k$ is also a commutative ring.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{definition}
  Let $R[X]$ be as above, and let $f_1, \ldots, f_n$ be polynomials in $R[X]$. We define
  \begin{equation*}
    \langle f_1, \ldots f_n \rangle = \left\{ \sum_{i = 1}^n h_i f_i \mid h_1, \ldots, h_n \in R[X] \right\}
  \end{equation*}
\end{definition}

Given polynomials $f_1, \ldots, f_n$, it is easy to see that $\langle f_1, \ldots, f_n \rangle$ is an ideal. Indeed, $h_1 = h_2 = \cdots = h_n = 0$, then $\sum h_i f_i = 0$, so $0 \in \langle f_1, \ldots, f_n \rangle$. For $f_j, f_k \in \{ f_1, \ldots, f_n \}$, set $h_i$ equal to $1$ if $i = j$ or $k$, and $0$ otherwise, so $f_j + f_k \in \langle f_1, \ldots, f_n \rangle$. For $f_k \in \{ f_1, \ldots, f_n \}$, set $h_i = c$ if $i = k$, and $0$ otherwise, so $c f_k \in \langle f_1, \ldots, f_n \rangle$. So, $\langle f_1, \ldots, f_n \rangle$ is an ideal in $R[X]$, namely, the \emph{ideal generated by $f_1, \ldots, f_n$}.

\begin{proposition}
  Let $k$ be a field, then the polynomial ring of a single variable over $k$, $k[x]$ is a Euclidean domain.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

Note that a polynomial ring of several variables is not a Euclidean domain.

\begin{example}
  \todo[inline]{Show that $k[x_1, \ldots, x_n]$ is not a Euclidean domain}
\end{example}

\section{Modules}
\label{sec:modules}

\begin{definition}
  Let $R$ be a ring. An \emph{$R$-module} $V$ is an abelian group with a law of composition written $+$, and a scalar multiplication $R \times V \to V$, written $r, v \mapsto rv$, such that
  \begin{equation}
  \label{eq:module-definition}
    1v = v \text{, } (rs)v = r(sv) \text{, } (r + s)v = rv + sv, \text{ and } r(v + v') = rv + rv'
  \end{equation}
  for all $r, s \in R, v, v' \in V$.
\end{definition}

\begin{example}
  Let $R$ be a ring. Then, $R$ is also an $R$-module, i.e.
  \begin{align*}
    a + b &= (a + b) * 1 \text{ and ,} \\
    ab &= (ab) * 1 \text{, }
  \end{align*}
  and so on.
\end{example}

\begin{definition}
  Let $V$ be an $R$-module, and let $H$ be a subgroup of $V$ that is also closed under scalar multiplication. In this case, we say that $H$, together with the laws of composition of $V$, is a \emph{submodule} of $V$.
\end{definition}

\todo[inline]{Discuss Isomorphism Theorems}

\begin{example}
  Let $R$ be a ring, and let $I \subset R$ be an ideal of $R$.

  \todo[inline]{Show that $I$ is a submodule of the $R$-module $R$}
\end{example}

\begin{definition}
  Let $V$ be a $R$-module. An ordered set $B = (v_1, \ldots, v_k)$, where $v_i \in V$ is said to generate $V$ if every element $v \in V$ can be written as a linear combination of the elements of B, i.e.
  \begin{equation*}
    v = r_1v_1 + \cdots + r_kv_k
  \end{equation*}
  where $r_i \in R$. Moreover, $V$ is said to be \emph{finitely generated} if there exists a finite set of generators.
\end{definition}

\todo[inline]{Discuss Free Modules}

\begin{proposition}[Ascending Chain Condition]
  \label{prop:acc}
  Let $M$ be an $R$-module, and let $\mathcal{M}$ be the set of submodules of $M$ partially ordered by inclusion (i.e. $M_i \subseteq M_j \land M_j \subseteq M_i \iff M_i = M_j$). Then, the following are equivalent
  \begin{enumerate}[i]
  \item \label{prop:acc-stationary} Every increasing sequence $M_1 \subseteq M_2 \subseteq \cdots \in \mathcal{M}$ is stationary (i.e. there exists $n$ such that $M_n = M_{n + 1} = \cdots$).
  \item \label{prop:acc-maximal} Every non-empty subset of $\mathcal{M}$ has a maximal element.
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{itemize}
  \item \cref{prop:acc-stationary} $implies$ \cref{prop:acc-maximal}
    If \cref{prop:acc-maximal} is false, then there is a non-empty subset $\mathcal{M'}$ of $\mathcal{M}$ with no maximal element. In this case, $\mathcal{M'}$ is a non-terminating, increasing sequence according to the relation $\subseteq$, which is a contradiction.
  \item \cref{prop:acc-maximal} $\implies$ \cref{prop:acc-stationary}
    Let $\mathcal{M'} \subset \mathcal{M}$ be nonempty. By \cref{prop:acc-stationary} $\mathcal{M'}$ terminates, at say, $M_n$, therefore $\mathcal{M'}$ has maximal element $M_n$.
  \end{itemize}
\end{proof}

A module $M$ satisfying \cref{prop:acc} is said to be \emph{Noetherian}. By \emph{Notherian ring}, we mean a ring $R$ that is Noetherian as an $R$-module.

\begin{proposition}
  Let $M$ be a Notherian $R$-module. Then, every submodule of $M$ is finitely generated.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{theorem}[Hilbert Basis Theorem]
  \label{thm:hilbert-basis}
  Let $R$ be a Noetherian ring. The polynomial ring $R[x]$ is Noetherian.
\end{theorem}

\begin{proof}
  TODO
\end{proof}

Note that by \cref{tm:hilbert-basis} a polynomial ring $k[x_1, \ldots, x_n]$ over a field $k$ is Noetherian.

\subsection{Gr\"oebner Bases}

\leavevmode

\todo[inline]{Discuss monomial orderings}

\todo{Paraphrase this - copied from Cox, Little \& O'Shea}
\begin{definition}
  Fix a monomial order. A finite subset $G = \{ g_1, \ldots, g_t \}$ of an ideal $I$ is said to be a \emph{Gr\"oebner basis} if
  \begin{equation*}
    \langle \mathrm{LT}(g_1), \ldots, \mathrm{LT(g_n)} \rangle = \langle \mathrm{LT}(I) \rangle
  \end{equation*}
\end{definition}

\begin{proposition}
  Let $k$ be a field, and let $k[x_1, \ldots, x_n]$ be the polynomial ring of $n$ variables over $k$ with a given monomial ordering. Then every ideal $I \subset k[x_1, \ldots, x_n]$, other than $\{ 0 \}$, has a Gr\"oebner basis $G$. Moreover, $G$ is a basis for $I$.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\todo[inline]{Discuss reduced Groebner bases, including uniqueness}

\begin{proposition}
  Let $G$ be a Gr\"oebner basis for $I \subset k[x_1, \ldots, x_n]$, where $k$ is a field, $G^R$, the reduced Gr\"oebner basis of $G$, can be computed as follows:
  \begin{enumerate}
    \item \todo[inline]{Show steps to compute reduced Gr\"oebner basis of $G$}
  \end{enumerate}
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\begin{corollary}
  Given a Gr\"oebner basis $G$ of size $n$, the computation to produce the reduced Gr\"oebner basis $G^R$ of $G$ is $\mathcal{O}(n)$. \todo{Find my notes discussing this}
\end{corollary}

\subsection{Matrix Representation of a Linear Operator on a Module}

In this section, let $R = k[x_1, \ldots, x_n]$ be the polynomial ring in $n$ variables over a field $k$, and let $M, N$ be an $R$-modules.

Similar to our discussion on Linear Operators on Vector Spaces (see \cref{sec:preliminaries}), given a linear operator $T: M \to N$, we can produce a \emph{matrix representation} of $T$. Given a basis $\{ e_i \}$ of $M$, i.e. the canonical basis $(0, \ldots, 1, \ldots, 0)$, arrange the mapping of $Te_i$ into a matrix, with $Te_i$ in the $i$th column.

\begin{figure}[h]
  \todo[inline]{Construct matrix representation of $T$, given basis $\{ e_i \}$}
\end{figure}

\subsection{Row Echelon Form of a Polynomial Matrix}

\begin{lstlisting}[caption=Computing the Row Echelon Form of a Polynomial Matrix]
    max_pivots = min(row_size, column_size)
    for i in (1 ... max_pivots):
        for j in (1 ... max_pivots):
            if i == j:
                continue

            target_lcm = lcm(matrix[i][i], matrix[j][i])

            pivot_quotient = target_lcm / matrix[i][i]
            clear_quotient = target_lcm / matrix[j][i]

            matrix.scala_mult(clear_quotient, j)
            matrix.subtract_row(pivot_quotient, i, j)
\end{lstlisting}

\section{When does a Polynomial Matrix have Rank $\leq$ n?}
\label{sec:polynomial-matrix}

\section{Examples}

\section{Conclusion}

\newpage

\section*{Notes from sources}
\label{sec:notes-from-sources}

\blockquote[S. Lang, Algebra, pg. 138]{It will be proved in the next section that a vector space over a field is always free, i.e. has a basis. Under certain circumstances, it is a theorem that projective modules are free. In \S 7 we shall prove that a finitely generated projective module over a principle ring is free. In Chapter X, Theorem 4.4 we shall prove that such a module over a local ring is free; in Chapter XVI, Theorem 3.8 we shall prove that a finite flat module over a local ring is free; and in Chapter XXI, Theorem 3.7, we shall prove the Quillen-Suslin theorem that if $A = k[X_1, \ldots, X_n]$ is the polynomial ring over a field $k$, then every finite projective module over $A$ is free.}

\newpage

\section*{Appendix}

\subsection*{Source Code}

\leavevmode \\

\todo[inline]{Include relevant code without inline documentation from https://github.com/patrickmclaren/math-thesis}

\subsection*{Code Documentation}

\leavevmode \\

\todo[inline]{Include documentation (ensure short and unstyled) from https://github.com/patrickmclaren/polyrank-docs}

\newpage

\printbibliography

\end{document}